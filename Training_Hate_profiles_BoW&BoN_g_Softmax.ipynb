{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training Hate profiles_BoW&BoN-g_Softmax.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMQOHlOjG9+mvrSkKlSyI+H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Josee0002/Machine_Learning/blob/main/Training_Hate_profiles_BoW%26BoN_g_Softmax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaUL5zSF7R_2"
      },
      "source": [
        "# **Jose √Ångel Pertuz Montes**\n",
        "# **Miguel √Ångel Banda del Valle**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CM-v2D96WbD_"
      },
      "source": [
        "**Conectar con Drive** - Acceder a datos preprocesados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0Cpl20pgF0k",
        "outputId": "65835f2b-6d28-4186-fd51-9acb526c7e9c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZ2qIOeEgsb3"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/ColabNotebooks/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcE3zfC_lauB"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "import spacy\n",
        "import unicodedata\n",
        "from nltk import TweetTokenizer\n",
        "from spacy.lang.es import Spanish\n",
        "from spacy.lang.en import English\n",
        "from nltk.util import ngrams\n",
        "\n",
        "\n",
        "class TextProcessing(object):\n",
        "    name = 'Text Processing'\n",
        "    lang = 'es'\n",
        "\n",
        "    def __init__(self, lang: str = 'es'):\n",
        "        self.lang = lang\n",
        "\n",
        "    @staticmethod\n",
        "    def proper_encoding(text: str):\n",
        "        result = ''\n",
        "        try:\n",
        "            text = unicodedata.normalize('NFD', text)\n",
        "            text = text.encode('ascii', 'ignore')\n",
        "            result = text.decode(\"utf-8\")\n",
        "        except Exception as e:\n",
        "            print('Error proper_encoding: {0}'.format(e))\n",
        "        return result\n",
        "\n",
        "    @staticmethod\n",
        "    def stopwords(text: str):\n",
        "        result = ''\n",
        "        try:\n",
        "            nlp = Spanish()if TextProcessing == 'es' else English()\n",
        "            doc = nlp(text)\n",
        "            token_list = [token.text for token in doc]\n",
        "            sentence = []\n",
        "            for word in token_list:\n",
        "                lexeme = nlp.vocab[word]\n",
        "                if not lexeme.is_stop:\n",
        "                    sentence.append(word)\n",
        "            result = ' '.join(sentence)\n",
        "        except Exception as e:\n",
        "            print('Error stopwords: {0}'.format(e))\n",
        "        return result\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_patterns(text: str):\n",
        "        result = ''\n",
        "        try:\n",
        "            text = re.sub(r'\\¬©|\\√ó|\\‚áî|\\_|\\¬ª|\\¬´|\\~|\\#|\\$|\\‚Ç¨|\\√Ç|\\ÔøΩ|\\¬¨', '', text)\n",
        "            text = re.sub(r'\\,|\\;|\\:|\\!|\\¬°|\\‚Äô|\\‚Äò|\\‚Äù|\\‚Äú|\\\"|\\'|\\`', '', text)\n",
        "            text = re.sub(r'\\}|\\{|\\[|\\]|\\(|\\)|\\<|\\>|\\?|\\¬ø|\\¬∞|\\|', '', text)\n",
        "            text = re.sub(r'\\/|\\-|\\+|\\*|\\=|\\^|\\%|\\&|\\$', '', text)\n",
        "            text = re.sub(r'\\b\\d+(?:\\.\\d+)?\\s+', '', text)\n",
        "            result = text.lower()\n",
        "        except Exception as e:\n",
        "            print('Error remove_patterns: {0}'.format(e))\n",
        "        return result\n",
        "\n",
        "    @staticmethod\n",
        "    def transformer(text: str, stopwords: bool = False):\n",
        "        result = ''\n",
        "        try:\n",
        "            text_out = TextProcessing.proper_encoding(text)\n",
        "            text_out = text_out.lower()\n",
        "            text_out = re.sub(\"[\\U0001f000-\\U000e007f]\", '[EMOJI]', text_out)\n",
        "            text_out = re.sub(\n",
        "                r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+'\n",
        "                r'|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?¬´¬ª‚Äú‚Äù‚Äò‚Äô]))',\n",
        "                '[URL]', text_out)\n",
        "            text_out = re.sub(\"@([A-Za-z0-9_]{1,40})\", '[MENTION]', text_out)\n",
        "            text_out = re.sub(\"#([A-Za-z0-9_]{1,40})\", '[HASTAG]', text_out)\n",
        "            text_out = TextProcessing.remove_patterns(text_out)\n",
        "            # text_out = TextAnalysis.lemmatization(text_out) if lemmatizer else text_out\n",
        "            text_out = TextProcessing.stopwords(text_out) if stopwords else text_out\n",
        "            text_out = re.sub(r'\\s+', ' ', text_out).strip()\n",
        "            text_out = text_out.rstrip()\n",
        "            result = text_out if text_out != ' ' else None\n",
        "        except Exception as e:\n",
        "            print('Error transformer: {0}'.format(e))\n",
        "        return result\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenizer(text: str):\n",
        "        val = []\n",
        "        try:\n",
        "            text_tokenizer = TweetTokenizer()\n",
        "            val = text_tokenizer.tokenize(text)\n",
        "        except Exception as e:\n",
        "            print('Error make_ngrams: {0}'.format(e))\n",
        "        return val\n",
        "\n",
        "    @staticmethod\n",
        "    def make_ngrams(text: str, num: int):\n",
        "        result = ''\n",
        "        try:\n",
        "            n_grams = ngrams(nltk.word_tokenize(text), num)\n",
        "            result = [' '.join(grams) for grams in n_grams]\n",
        "        except Exception as e:\n",
        "            print('Error make_ngrams: {0}'.format(e))\n",
        "        return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZBe53diZh8j"
      },
      "source": [
        "def stopwords(text: str):\n",
        "        result = ''\n",
        "        try:\n",
        "            nlp = Spanish()if TextProcessing == 'es' else English()\n",
        "            doc = nlp(text)\n",
        "            token_list = [token.text for token in doc]\n",
        "            sentence = []\n",
        "            for word in token_list:\n",
        "                lexeme = nlp.vocab[word]\n",
        "                if not lexeme.is_stop:\n",
        "                    sentence.append(word)\n",
        "            result = ' '.join(sentence)\n",
        "        except Exception as e:\n",
        "            print('Error stopwords: {0}'.format(e))\n",
        "        return result\n",
        "\n",
        "def transformer(text: str, stopwords: bool = False):\n",
        "        result = ''\n",
        "        try:\n",
        "            text_out = TextProcessing.proper_encoding(text)\n",
        "            text_out = text_out.lower()\n",
        "            text_out = re.sub(\"[\\U0001f000-\\U000e007f]\", '[EMOJI]', text_out)\n",
        "            text_out = re.sub(\n",
        "                r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+'\n",
        "                r'|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?¬´¬ª‚Äú‚Äù‚Äò‚Äô]))',\n",
        "                '[URL]', text_out)\n",
        "            text_out = re.sub(\"@([A-Za-z0-9_]{1,40})\", '[MENTION]', text_out)\n",
        "            text_out = re.sub(\"#([A-Za-z0-9_]{1,40})\", '[HASTAG]', text_out)\n",
        "            text_out = TextProcessing.remove_patterns(text_out)\n",
        "            # text_out = TextAnalysis.lemmatization(text_out) if lemmatizer else text_out\n",
        "            text_out = TextProcessing.stopwords(text_out) if stopwords else text_out\n",
        "            text_out = re.sub(r'\\s+', ' ', text_out).strip()\n",
        "            text_out = text_out.rstrip()\n",
        "            result = text_out if text_out != ' ' else None\n",
        "        except Exception as e:\n",
        "            print('Error transformer: {0}'.format(e))\n",
        "        return result\n",
        "def make_ngrams(text: str, num: int):\n",
        "        result = ''\n",
        "        try:\n",
        "            n_grams = ngrams(nltk.word_tokenize(text), num)\n",
        "            result = [' '.join(grams) for grams in n_grams]\n",
        "        except Exception as e:\n",
        "            print('Error make_ngrams: {0}'.format(e))\n",
        "        return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pITcmWfFnvIt"
      },
      "source": [
        "import io\n",
        "import sys\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "from collections import Counter\n",
        "from sklearn.preprocessing import LabelEncoder \n",
        "from sklearn import preprocessing\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89-gZb3-6C-d"
      },
      "source": [
        "le = LabelEncoder()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "xxuy-KA_jAY3",
        "outputId": "6de1d173-c2f7-48dc-c3cd-0f9b03e4df0b"
      },
      "source": [
        "raw_data = pd.read_csv('/content/drive/MyDrive/Datasets/Hate_Profile.csv', sep=';')\n",
        "raw_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>comment</th>\n",
              "      <th>polarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0035a3060d075506f5b9b978a910aa1f</td>\n",
              "      <td>pasta con bichos de agua</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0035a3060d075506f5b9b978a910aa1f</td>\n",
              "      <td>De verdad puto lol de mierda qu√© asco de juego...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0035a3060d075506f5b9b978a910aa1f</td>\n",
              "      <td>me hice una pcr y ya tengo los resultados! m...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0035a3060d075506f5b9b978a910aa1f</td>\n",
              "      <td>Y un lomo queso de baguette entera, tranqui</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0035a3060d075506f5b9b978a910aa1f</td>\n",
              "      <td>Me cambio de curro y me llegan 3 ofeas directa...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39995</th>\n",
              "      <td>fde1d7437a12068e0e39505af6948f99</td>\n",
              "      <td>Herrera, a Iglesias \"Es una anomal√≠a democr√°...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39996</th>\n",
              "      <td>fde1d7437a12068e0e39505af6948f99</td>\n",
              "      <td>Dice el hij√∏puta de Otegi que lo m√°s bonito ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39997</th>\n",
              "      <td>fde1d7437a12068e0e39505af6948f99</td>\n",
              "      <td>I√±aqui,  lo que opines de  nos la trae al pa...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39998</th>\n",
              "      <td>fde1d7437a12068e0e39505af6948f99</td>\n",
              "      <td>Si te gusta alguna de las putas que salen en...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39999</th>\n",
              "      <td>fde1d7437a12068e0e39505af6948f99</td>\n",
              "      <td>Se ofrece tarotista fiable, serio, con exper...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>40000 rows √ó 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     Id  ... polarity\n",
              "0      0035a3060d075506f5b9b978a910aa1f  ...        0\n",
              "1      0035a3060d075506f5b9b978a910aa1f  ...        0\n",
              "2      0035a3060d075506f5b9b978a910aa1f  ...        0\n",
              "3      0035a3060d075506f5b9b978a910aa1f  ...        0\n",
              "4      0035a3060d075506f5b9b978a910aa1f  ...        0\n",
              "...                                 ...  ...      ...\n",
              "39995  fde1d7437a12068e0e39505af6948f99  ...        1\n",
              "39996  fde1d7437a12068e0e39505af6948f99  ...        1\n",
              "39997  fde1d7437a12068e0e39505af6948f99  ...        1\n",
              "39998  fde1d7437a12068e0e39505af6948f99  ...        1\n",
              "39999  fde1d7437a12068e0e39505af6948f99  ...        1\n",
              "\n",
              "[40000 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEw1l15t7ytT"
      },
      "source": [
        "raw=[]\n",
        "Id=[] \n",
        "comment=[]\n",
        "polarity=[]\n",
        "for i in range(200):\n",
        "  Id.append(raw_data['Id'][200*i])\n",
        "  polarity.append(raw_data['polarity'][200*i])\n",
        "  comment_sub=[]\n",
        "  for j in range(200):\n",
        "    comment_sub.append(raw_data['comment'][j+200*i])\n",
        "  b=np.transpose(comment_sub)\n",
        "  comment.append(b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLPWaohf8pGY",
        "outputId": "955ccb63-f13d-474e-f990-3a89b3000f06"
      },
      "source": [
        "np.array(comment)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[' pasta con bichos de agua',\n",
              "        'De verdad puto lol de mierda qu√© asco de juego joder',\n",
              "        '  me hice una pcr y ya tengo los resultados! menos mal ', ...,\n",
              "        ' La mayoria de estos los teneis en el PV fkskdkdkdk',\n",
              "        ' Full ruta del bakalao tocadisimo por las drogas duras y el hardcore ü§£ü§£ü§£',\n",
              "        ' Va desayuno y sms'],\n",
              "       ['  Este es, de manera objetiva, el mejor ejemplo que he visto a d√≠a de hoy respecto de la desechabilidad masculina.',\n",
              "        '  A los hosteleros les estar√° haciendo much√≠sima gracia ',\n",
              "        '  El discurso vs la realidad ', ...,\n",
              "        '  Eso no se llama cafe ‚Äúbombon‚Äù?!üòÇ', '  Que cojones es eso?',\n",
              "        '  ‚ÄúNo me toques que me lo engrasas‚Äùü§®'],\n",
              "       [' Uno de los mejores controladores que conozco y de las mejores personas que conozco',\n",
              "        ' Pan de boda, duro a las pocas horas',\n",
              "        ' La cinta y las comisiones peinentes, porque por ese precio ..',\n",
              "        ...,\n",
              "        '  Yo a√±adir√≠a que es la abreviatura de \"ropa de vestir elegante\".',\n",
              "        ' Que credibilidad tiene esta delincuente?',\n",
              "        ' Que es idiota? Ya lo sabemos'],\n",
              "       ...,\n",
              "       [' Lo mejor que he visto en tiempos',\n",
              "        '  Irene Montero paseando junto al resto de ministras de Igualdad de la UE tras su √∫ltima reuni√≥n de trabajo.',\n",
              "        '  Hoy hay que recordarlo. ', ...,\n",
              "        ' Le ha faltado sonarse los mocos....que asco de china....',\n",
              "        ' Van a cerrar las sedes de Podemos?',\n",
              "        ' Y adem√°s es de buen comer.'],\n",
              "       [' Omg esa canci√≥n esta bn bonita, hay q dedic√°rnosla üëÄüëÄ',\n",
              "        '  ‚ÄúSe escandalizan por las que luchan y no por las que mueren‚Äù',\n",
              "        '  Ni siquiera se compoaban en los honores a la bandera y ahora le lloran a monumentos de los que no conocen de q son',\n",
              "        ...,\n",
              "        'Pensando si hacer la tarea de calculo,,,, like y no la hago',\n",
              "        ' Eee tambi√©n pense eso as√≠ de q the society we, pero no me re√≠ üòÄ',\n",
              "        'La profa de ingl√©s esta obsesionada con decir mi segundo nombre en ingl√©s, p√°renla'],\n",
              "       ['  Izquierda Moci√≥n de censura. Derecha Elecciones democr√°ticas. ¬øSe entiende la diferencia, verdad?',\n",
              "        '  La Lastra y el lastre ',\n",
              "        '  El 4 de Mayo t√∫ eliges. O un Madrid de libead O un estercolero como Barcelona ',\n",
              "        ...,\n",
              "        '  I√±aqui,  lo que opines de  nos la trae al pairo. Preferimos que no escupas a tu novia, gracias.',\n",
              "        '  Si te gusta alguna de las putas que salen en Telecirco, b√∫scalas en Tinder. M√°s claro no te lo pueden decir.',\n",
              "        '  Se ofrece tarotista fiable, serio, con experiencia, Fiable 100%. \"LLAMA AHORA\" ']],\n",
              "      dtype='<U141')"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3tOHIg486QV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fcae43d-8606-4dab-8983-51923d2b77e5"
      },
      "source": [
        "np.shape(comment)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(200, 200)"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0CYqZeuj-cB",
        "outputId": "ce942df1-2590-4a09-8a7b-a4610e37f6b2"
      },
      "source": [
        "y = le.fit_transform(raw_data['polarity'])\n",
        "y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 1, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUPUVhUxuKEO"
      },
      "source": [
        "# **Bag of Words**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLRbItyPeQOa"
      },
      "source": [
        "The **bag-of-words (BOW)** model represents a conversion or transformation of text into fixed-length vectors, which identifies the frequency of occurrence of each word within the text, a process called vectorization.\n",
        "\n",
        "It is a classification model whose strong point is its simplicity, since it demands little programming cost, and it is used in Natural Language Processing for text classification, in occasions where within a set of texts there is contextual information that is not relevant, simplicity turns out to be the best tool within this type of analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1giQNmX78chE"
      },
      "source": [
        "corpus = [transformer(row) for row in raw_data['comment'].tolist()]\n",
        "bow = CountVectorizer(analyzer='word', ngram_range=(1, 1))\n",
        "x = bow.fit_transform(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQitAqYmTSoy"
      },
      "source": [
        "#df = pd.DataFrame(x.toarray(),\n",
        "#                 index=['content '+str(i) for i in range(1, 1+len(corpus))],\n",
        "#                 columns=bow.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2nvc9ZlTwFo"
      },
      "source": [
        "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.25,random_state=40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBAnYvBjUHeD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c21bb0b6-128d-4895-d036-af96079ac85f"
      },
      "source": [
        "softmax = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", C=10, max_iter=200)\n",
        "softmax.fit(x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=200,\n",
              "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIOil6pDbOo1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3389169d-83e3-444e-c6f2-883f409aa40f"
      },
      "source": [
        "y_predict = softmax.predict(x_test)\n",
        "confusion_matrix(y_test, y_predict, labels=[0, 1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3264, 1746],\n",
              "       [1744, 3246]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZxNHcQ-cftX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3d012e5-2257-478c-95d0-98ae17538e7c"
      },
      "source": [
        "classification = classification_report(y_test, y_predict)\n",
        "print(classification)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.65      0.65      5010\n",
            "           1       0.65      0.65      0.65      4990\n",
            "\n",
            "    accuracy                           0.65     10000\n",
            "   macro avg       0.65      0.65      0.65     10000\n",
            "weighted avg       0.65      0.65      0.65     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YR-3djVduBBL"
      },
      "source": [
        "# **Bag of N-grams**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7tt-SPFcHNL"
      },
      "source": [
        "The classical **N-gram model** is one of the easiest notions to understand in the machine learning space.\n",
        "\n",
        "In the first instance, an N-gram is an N-word order, for example, \"Bad comment\" is a bigram, \"Bad comment\" is a trigram, and \"It was a bad comment\" is a 4-gram, and so on.\n",
        "\n",
        "The use of N-Grams features can be useful for improving the classification performance of natural language processing, whether for automatic spell checking, sentence auto-completion, and also takes field in the analysis of expressed sentiments.\n",
        "\n",
        "If we take the expression \"No, I like your blouse\" it would represent a positive sentiment, but if we take it this way, \"I don't like your blouse\" it would already be a negative sentiment, unlike the \"Bag of Words\" model, this one would be able to identify the difference between one expression and another by studying n-words as a whole and not at the individual level."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGohUMOftUaU",
        "outputId": "c52a159a-79b6-4226-baaf-eb7237c1c861"
      },
      "source": [
        "corpus = [transformer(row) for row in raw_data['comment'].tolist()]\n",
        "bow = CountVectorizer(analyzer='word', ngram_range=(1, 3))\n",
        "x = bow.fit_transform(corpus)\n",
        "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.25,random_state=40)\n",
        "softmax = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", C=10, max_iter=200)\n",
        "softmax.fit(x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=200,\n",
              "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evB7-s46vPQR",
        "outputId": "9bb22a7f-a491-409b-a58a-05050f446bab"
      },
      "source": [
        "y_predict = softmax.predict(x_test)\n",
        "confusion_matrix(y_test, y_predict, labels=[0, 1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3378, 1632],\n",
              "       [1676, 3314]])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URlwEUsXvQup",
        "outputId": "32f3572a-4232-4c4f-a788-7a3d7c1847d4"
      },
      "source": [
        "classification = classification_report(y_test, y_predict)\n",
        "print(classification)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.67      0.67      5010\n",
            "           1       0.67      0.66      0.67      4990\n",
            "\n",
            "    accuracy                           0.67     10000\n",
            "   macro avg       0.67      0.67      0.67     10000\n",
            "weighted avg       0.67      0.67      0.67     10000\n",
            "\n"
          ]
        }
      ]
    }
  ]
}