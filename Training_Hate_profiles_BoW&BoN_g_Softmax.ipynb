{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training Hate profiles_BoW&BoN-g_Softmax.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNINL31PV+muldbH0Y5jCOd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Josee0002/Machine_Learning/blob/main/Training_Hate_profiles_BoW%26BoN_g_Softmax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CM-v2D96WbD_"
      },
      "source": [
        "**Conectar con Drive** - Acceder a datos preprocesados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0Cpl20pgF0k",
        "outputId": "65835f2b-6d28-4186-fd51-9acb526c7e9c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZ2qIOeEgsb3"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/ColabNotebooks/')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcE3zfC_lauB"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "import spacy\n",
        "import unicodedata\n",
        "from nltk import TweetTokenizer\n",
        "from spacy.lang.es import Spanish\n",
        "from spacy.lang.en import English\n",
        "from nltk.util import ngrams\n",
        "\n",
        "\n",
        "class TextProcessing(object):\n",
        "    name = 'Text Processing'\n",
        "    lang = 'es'\n",
        "\n",
        "    def __init__(self, lang: str = 'es'):\n",
        "        self.lang = lang\n",
        "\n",
        "    @staticmethod\n",
        "    def proper_encoding(text: str):\n",
        "        result = ''\n",
        "        try:\n",
        "            text = unicodedata.normalize('NFD', text)\n",
        "            text = text.encode('ascii', 'ignore')\n",
        "            result = text.decode(\"utf-8\")\n",
        "        except Exception as e:\n",
        "            print('Error proper_encoding: {0}'.format(e))\n",
        "        return result\n",
        "\n",
        "    @staticmethod\n",
        "    def stopwords(text: str):\n",
        "        result = ''\n",
        "        try:\n",
        "            nlp = Spanish()if TextProcessing == 'es' else English()\n",
        "            doc = nlp(text)\n",
        "            token_list = [token.text for token in doc]\n",
        "            sentence = []\n",
        "            for word in token_list:\n",
        "                lexeme = nlp.vocab[word]\n",
        "                if not lexeme.is_stop:\n",
        "                    sentence.append(word)\n",
        "            result = ' '.join(sentence)\n",
        "        except Exception as e:\n",
        "            print('Error stopwords: {0}'.format(e))\n",
        "        return result\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_patterns(text: str):\n",
        "        result = ''\n",
        "        try:\n",
        "            text = re.sub(r'\\©|\\×|\\⇔|\\_|\\»|\\«|\\~|\\#|\\$|\\€|\\Â|\\�|\\¬', '', text)\n",
        "            text = re.sub(r'\\,|\\;|\\:|\\!|\\¡|\\’|\\‘|\\”|\\“|\\\"|\\'|\\`', '', text)\n",
        "            text = re.sub(r'\\}|\\{|\\[|\\]|\\(|\\)|\\<|\\>|\\?|\\¿|\\°|\\|', '', text)\n",
        "            text = re.sub(r'\\/|\\-|\\+|\\*|\\=|\\^|\\%|\\&|\\$', '', text)\n",
        "            text = re.sub(r'\\b\\d+(?:\\.\\d+)?\\s+', '', text)\n",
        "            result = text.lower()\n",
        "        except Exception as e:\n",
        "            print('Error remove_patterns: {0}'.format(e))\n",
        "        return result\n",
        "\n",
        "    @staticmethod\n",
        "    def transformer(text: str, stopwords: bool = False):\n",
        "        result = ''\n",
        "        try:\n",
        "            text_out = TextProcessing.proper_encoding(text)\n",
        "            text_out = text_out.lower()\n",
        "            text_out = re.sub(\"[\\U0001f000-\\U000e007f]\", '[EMOJI]', text_out)\n",
        "            text_out = re.sub(\n",
        "                r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+'\n",
        "                r'|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))',\n",
        "                '[URL]', text_out)\n",
        "            text_out = re.sub(\"@([A-Za-z0-9_]{1,40})\", '[MENTION]', text_out)\n",
        "            text_out = re.sub(\"#([A-Za-z0-9_]{1,40})\", '[HASTAG]', text_out)\n",
        "            text_out = TextProcessing.remove_patterns(text_out)\n",
        "            # text_out = TextAnalysis.lemmatization(text_out) if lemmatizer else text_out\n",
        "            text_out = TextProcessing.stopwords(text_out) if stopwords else text_out\n",
        "            text_out = re.sub(r'\\s+', ' ', text_out).strip()\n",
        "            text_out = text_out.rstrip()\n",
        "            result = text_out if text_out != ' ' else None\n",
        "        except Exception as e:\n",
        "            print('Error transformer: {0}'.format(e))\n",
        "        return result\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenizer(text: str):\n",
        "        val = []\n",
        "        try:\n",
        "            text_tokenizer = TweetTokenizer()\n",
        "            val = text_tokenizer.tokenize(text)\n",
        "        except Exception as e:\n",
        "            print('Error make_ngrams: {0}'.format(e))\n",
        "        return val\n",
        "\n",
        "    @staticmethod\n",
        "    def make_ngrams(text: str, num: int):\n",
        "        result = ''\n",
        "        try:\n",
        "            n_grams = ngrams(nltk.word_tokenize(text), num)\n",
        "            result = [' '.join(grams) for grams in n_grams]\n",
        "        except Exception as e:\n",
        "            print('Error make_ngrams: {0}'.format(e))\n",
        "        return result"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZBe53diZh8j"
      },
      "source": [
        "def stopwords(text: str):\n",
        "        result = ''\n",
        "        try:\n",
        "            nlp = Spanish()if TextProcessing == 'es' else English()\n",
        "            doc = nlp(text)\n",
        "            token_list = [token.text for token in doc]\n",
        "            sentence = []\n",
        "            for word in token_list:\n",
        "                lexeme = nlp.vocab[word]\n",
        "                if not lexeme.is_stop:\n",
        "                    sentence.append(word)\n",
        "            result = ' '.join(sentence)\n",
        "        except Exception as e:\n",
        "            print('Error stopwords: {0}'.format(e))\n",
        "        return result\n",
        "\n",
        "def transformer(text: str, stopwords: bool = False):\n",
        "        result = ''\n",
        "        try:\n",
        "            text_out = TextProcessing.proper_encoding(text)\n",
        "            text_out = text_out.lower()\n",
        "            text_out = re.sub(\"[\\U0001f000-\\U000e007f]\", '[EMOJI]', text_out)\n",
        "            text_out = re.sub(\n",
        "                r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+'\n",
        "                r'|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))',\n",
        "                '[URL]', text_out)\n",
        "            text_out = re.sub(\"@([A-Za-z0-9_]{1,40})\", '[MENTION]', text_out)\n",
        "            text_out = re.sub(\"#([A-Za-z0-9_]{1,40})\", '[HASTAG]', text_out)\n",
        "            text_out = TextProcessing.remove_patterns(text_out)\n",
        "            # text_out = TextAnalysis.lemmatization(text_out) if lemmatizer else text_out\n",
        "            text_out = TextProcessing.stopwords(text_out) if stopwords else text_out\n",
        "            text_out = re.sub(r'\\s+', ' ', text_out).strip()\n",
        "            text_out = text_out.rstrip()\n",
        "            result = text_out if text_out != ' ' else None\n",
        "        except Exception as e:\n",
        "            print('Error transformer: {0}'.format(e))\n",
        "        return result\n",
        "def make_ngrams(text: str, num: int):\n",
        "        result = ''\n",
        "        try:\n",
        "            n_grams = ngrams(nltk.word_tokenize(text), num)\n",
        "            result = [' '.join(grams) for grams in n_grams]\n",
        "        except Exception as e:\n",
        "            print('Error make_ngrams: {0}'.format(e))\n",
        "        return result"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pITcmWfFnvIt"
      },
      "source": [
        "import io\n",
        "import sys\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "from collections import Counter\n",
        "from sklearn.preprocessing import LabelEncoder \n",
        "from sklearn import preprocessing\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89-gZb3-6C-d"
      },
      "source": [
        "le = LabelEncoder()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "xxuy-KA_jAY3",
        "outputId": "6de1d173-c2f7-48dc-c3cd-0f9b03e4df0b"
      },
      "source": [
        "raw_data = pd.read_csv('/content/drive/MyDrive/Datasets/Hate_Profile.csv', sep=';')\n",
        "raw_data"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>comment</th>\n",
              "      <th>polarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0035a3060d075506f5b9b978a910aa1f</td>\n",
              "      <td>pasta con bichos de agua</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0035a3060d075506f5b9b978a910aa1f</td>\n",
              "      <td>De verdad puto lol de mierda qué asco de juego...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0035a3060d075506f5b9b978a910aa1f</td>\n",
              "      <td>me hice una pcr y ya tengo los resultados! m...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0035a3060d075506f5b9b978a910aa1f</td>\n",
              "      <td>Y un lomo queso de baguette entera, tranqui</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0035a3060d075506f5b9b978a910aa1f</td>\n",
              "      <td>Me cambio de curro y me llegan 3 ofeas directa...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39995</th>\n",
              "      <td>fde1d7437a12068e0e39505af6948f99</td>\n",
              "      <td>Herrera, a Iglesias \"Es una anomalía democrá...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39996</th>\n",
              "      <td>fde1d7437a12068e0e39505af6948f99</td>\n",
              "      <td>Dice el hijøputa de Otegi que lo más bonito ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39997</th>\n",
              "      <td>fde1d7437a12068e0e39505af6948f99</td>\n",
              "      <td>Iñaqui,  lo que opines de  nos la trae al pa...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39998</th>\n",
              "      <td>fde1d7437a12068e0e39505af6948f99</td>\n",
              "      <td>Si te gusta alguna de las putas que salen en...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39999</th>\n",
              "      <td>fde1d7437a12068e0e39505af6948f99</td>\n",
              "      <td>Se ofrece tarotista fiable, serio, con exper...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>40000 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     Id  ... polarity\n",
              "0      0035a3060d075506f5b9b978a910aa1f  ...        0\n",
              "1      0035a3060d075506f5b9b978a910aa1f  ...        0\n",
              "2      0035a3060d075506f5b9b978a910aa1f  ...        0\n",
              "3      0035a3060d075506f5b9b978a910aa1f  ...        0\n",
              "4      0035a3060d075506f5b9b978a910aa1f  ...        0\n",
              "...                                 ...  ...      ...\n",
              "39995  fde1d7437a12068e0e39505af6948f99  ...        1\n",
              "39996  fde1d7437a12068e0e39505af6948f99  ...        1\n",
              "39997  fde1d7437a12068e0e39505af6948f99  ...        1\n",
              "39998  fde1d7437a12068e0e39505af6948f99  ...        1\n",
              "39999  fde1d7437a12068e0e39505af6948f99  ...        1\n",
              "\n",
              "[40000 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0CYqZeuj-cB",
        "outputId": "ce942df1-2590-4a09-8a7b-a4610e37f6b2"
      },
      "source": [
        "y = le.fit_transform(raw_data['polarity'])\n",
        "y"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 1, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUPUVhUxuKEO"
      },
      "source": [
        "# **Bag of Words**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1giQNmX78chE"
      },
      "source": [
        "corpus = [transformer(row) for row in raw_data['comment'].tolist()]\n",
        "bow = CountVectorizer(analyzer='word', ngram_range=(1, 1))\n",
        "x = bow.fit_transform(corpus)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQitAqYmTSoy"
      },
      "source": [
        "#df = pd.DataFrame(x.toarray(),\n",
        "#                 index=['content '+str(i) for i in range(1, 1+len(corpus))],\n",
        "#                 columns=bow.get_feature_names())"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2nvc9ZlTwFo"
      },
      "source": [
        "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.25,random_state=40)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBAnYvBjUHeD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c21bb0b6-128d-4895-d036-af96079ac85f"
      },
      "source": [
        "softmax = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", C=10, max_iter=200)\n",
        "softmax.fit(x_train, y_train)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=200,\n",
              "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIOil6pDbOo1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3389169d-83e3-444e-c6f2-883f409aa40f"
      },
      "source": [
        "y_predict = softmax.predict(x_test)\n",
        "confusion_matrix(y_test, y_predict, labels=[0, 1])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3264, 1746],\n",
              "       [1744, 3246]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZxNHcQ-cftX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3d012e5-2257-478c-95d0-98ae17538e7c"
      },
      "source": [
        "classification = classification_report(y_test, y_predict)\n",
        "print(classification)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.65      0.65      5010\n",
            "           1       0.65      0.65      0.65      4990\n",
            "\n",
            "    accuracy                           0.65     10000\n",
            "   macro avg       0.65      0.65      0.65     10000\n",
            "weighted avg       0.65      0.65      0.65     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YR-3djVduBBL"
      },
      "source": [
        "# **Bag of N-grams**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGohUMOftUaU",
        "outputId": "c52a159a-79b6-4226-baaf-eb7237c1c861"
      },
      "source": [
        "corpus = [transformer(row) for row in raw_data['comment'].tolist()]\n",
        "bow = CountVectorizer(analyzer='word', ngram_range=(1, 3))\n",
        "x = bow.fit_transform(corpus)\n",
        "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.25,random_state=40)\n",
        "softmax = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", C=10, max_iter=200)\n",
        "softmax.fit(x_train, y_train)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=200,\n",
              "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evB7-s46vPQR",
        "outputId": "9bb22a7f-a491-409b-a58a-05050f446bab"
      },
      "source": [
        "y_predict = softmax.predict(x_test)\n",
        "confusion_matrix(y_test, y_predict, labels=[0, 1])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3378, 1632],\n",
              "       [1676, 3314]])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URlwEUsXvQup",
        "outputId": "32f3572a-4232-4c4f-a788-7a3d7c1847d4"
      },
      "source": [
        "classification = classification_report(y_test, y_predict)\n",
        "print(classification)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.67      0.67      5010\n",
            "           1       0.67      0.66      0.67      4990\n",
            "\n",
            "    accuracy                           0.67     10000\n",
            "   macro avg       0.67      0.67      0.67     10000\n",
            "weighted avg       0.67      0.67      0.67     10000\n",
            "\n"
          ]
        }
      ]
    }
  ]
}